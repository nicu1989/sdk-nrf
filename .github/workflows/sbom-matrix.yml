name: Parallel SBOM

on:
  pull_request:
  workflow_dispatch:
    inputs:
      chunk_size:
        description: Target number of files per job (auto-increases to stay â‰¤200 jobs)
        required: false
        default: '500'

permissions:
  contents: read

env:
  SBOM_REQUIREMENTS_FILE: scripts/requirements-west-ncs-sbom.txt
  EXTRA_REQUIREMENTS_FILE: scripts/requirements-extra.txt
  MAX_MATRIX_JOBS: '200'

jobs:
  prepare:
    name: Prepare matrix
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.compute.outputs.matrix }}
      chunk_size: ${{ steps.compute.outputs.chunk_size }}
      chunk_count: ${{ steps.compute.outputs.chunk_count }}
      total_files: ${{ steps.compute.outputs.total_files }}
    steps:
      - name: Checkout
        uses: nrfconnect/action-checkout-west-update@main
        with:
          path: ncs/nrf
          git-fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@42375524e23c412d93fb67b49958b491fce71c38 # v5
        with:
          python-version: '3.12'

      - name: Install python dependencies
        working-directory: ncs/nrf
        run: |
          pip3 install -U mypy types-colorama types-editdistance types-PyYAML
          grep -E "west==" scripts/requirements-fixed.txt | cut -f1 -d"#" | cut -d ' ' -f '1'| xargs pip3 install -U
          pip3 show -f west

      - id: compute
        name: Compute workload matrix
        env:
          REQUESTED_CHUNK_SIZE: ${{ inputs.chunk_size }}
        working-directory: ncs/nrf
        run: |
          python <<'PY'
          import json
          import math
          import os
          import subprocess

          requested = os.environ.get('REQUESTED_CHUNK_SIZE', '').strip()
          try:
              requested_size = int(requested)
          except ValueError:
              requested_size = 500
          if requested_size <= 0:
              requested_size = 500

          files = subprocess.check_output(['git', 'ls-files'], text=True).splitlines()
          total_files = len(files)
          max_jobs = int(os.environ['MAX_MATRIX_JOBS'])

          if total_files == 0:
              chunk_size = requested_size
              chunk_count = 0
          else:
              min_chunk = max(1, math.ceil(total_files / max_jobs))
              chunk_size = max(requested_size, min_chunk)
              chunk_count = math.ceil(total_files / chunk_size)

          matrix = {'include': [{'chunk_id': idx} for idx in range(chunk_count)]}

          with open(os.environ['GITHUB_OUTPUT'], 'a', encoding='utf-8') as fh:
              fh.write(f'chunk_size={chunk_size}\n')
              fh.write(f'chunk_count={chunk_count}\n')
              fh.write(f'total_files={total_files}\n')
              fh.write('matrix<<EOF\n')
              fh.write(json.dumps(matrix))
              fh.write('\nEOF\n')
          PY

  scan:
    name: Scan chunk ${{ matrix.chunk_id }}
    needs: prepare
    if: needs.prepare.outputs.chunk_count != '0'
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.prepare.outputs.matrix) }}
    steps:
      - name: Checkout
        uses: nrfconnect/action-checkout-west-update@main
        with:
          path: ncs/nrf
          git-fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@42375524e23c412d93fb67b49958b491fce71c38 # v5
        with:
          python-version: '3.12'

      - name: Install python dependencies
        working-directory: ncs/nrf
        run: |
          pip3 install -U mypy types-colorama types-editdistance types-PyYAML
          grep -E "west==" scripts/requirements-fixed.txt | cut -f1 -d"#" | cut -d ' ' -f '1'| xargs pip3 install -U
          pip3 show -f west

      - name: Install requirements
        shell: bash
        working-directory: ncs/nrf
        run: |
          pip3 install -r scripts/requirements-west-ncs-sbom.txt

      - id: select
        name: Select files for this chunk
        working-directory: ncs/nrf
        env:
          CHUNK_ID: ${{ matrix.chunk_id }}
          CHUNK_SIZE: ${{ needs.prepare.outputs.chunk_size }}
        run: |
          python <<'PY'
          import os
          import subprocess

          chunk_id = int(os.environ['CHUNK_ID'])
          chunk_size = int(os.environ['CHUNK_SIZE'])

          data = subprocess.check_output(['git', 'ls-files', '-z'])
          files = [entry for entry in data.decode().split('\0') if entry]

          start = chunk_id * chunk_size
          end = min(len(files), start + chunk_size)

          if start >= len(files):
              raise SystemExit(f'Chunk {chunk_id} starts beyond available files.')

          selected = files[start:end]
          output = f'sbom-chunk-{chunk_id}.txt'
          with open(output, 'w', encoding='utf-8') as fp:
              for path in selected:
                  fp.write(f'{path}\n')

          print(f'Chunk {chunk_id}: {len(selected)} files ({start} to {end - 1})')
          PY

      - name: Run ncs-sbom with scancode-toolkit only
        working-directory: ncs/nrf
        env:
          CHUNK_ID: ${{ matrix.chunk_id }}
        run: |
          west ncs-sbom \
            --input-list-file "sbom-chunk-${CHUNK_ID}.txt" \
            --license-detectors scancode-toolkit \
            --optional-license-detectors scancode-toolkit \
            --output-cache-database "sbom-chunk-${CHUNK_ID}.json"

      - name: Upload chunk cache
        uses: actions/upload-artifact@v4
        with:
          name: sbom-chunk-${{ matrix.chunk_id }}-cache
          if-no-files-found: error
          retention-days: 7
          path: |
            ncs/nrf/sbom-chunk-${{ matrix.chunk_id }}.json
            ncs/nrf/sbom-chunk-${{ matrix.chunk_id }}.txt

  aggregate:
    name: Aggregate reports
    needs:
      - prepare
      - scan
    if: needs.prepare.outputs.chunk_count != '0'
    runs-on: ubuntu-latest
    timeout-minutes: 360
    steps:
      - name: Checkout
        uses: nrfconnect/action-checkout-west-update@main
        with:
          path: ncs/nrf
          git-fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@42375524e23c412d93fb67b49958b491fce71c38 # v5
        with:
          python-version: '3.12'

      - name: Install OS prerequisites
        run: |
          sudo apt-get update
          sudo apt-get install -y python3-dev bzip2 xz-utils zlib1g libxml2-dev libxslt1-dev libpopt0

      - name: Install python dependencies
        working-directory: ncs/nrf
        run: |
          pip3 install -U mypy types-colorama types-editdistance types-PyYAML
          grep -E "west==" scripts/requirements-fixed.txt | cut -f1 -d"#" | cut -d ' ' -f '1'| xargs pip3 install -U
          pip3 show -f west

      - name: Install requirements
        shell: bash
        working-directory: ncs/nrf
        run: |
          pip3 install -r scripts/requirements-west-ncs-sbom.txt  

      - name: Download chunk caches
        uses: actions/download-artifact@v4
        with:
          pattern: sbom-chunk-*-cache
          path: ncs/nrf/chunk-caches
          merge-multiple: true

      - name: Repair invalid chunk caches if needed
        working-directory: ncs/nrf
        env:
          EXPECTED_CHUNKS: ${{ needs.prepare.outputs.chunk_count }}
        run: |
          python <<'PY'
          import json
          import os
          import pathlib
          import re
          import subprocess

          expected = int(os.environ['EXPECTED_CHUNKS'])
          cache_dir = pathlib.Path('chunk-caches')
          cache_dir.mkdir(exist_ok=True)

          cache_paths = sorted(cache_dir.glob('sbom-chunk-*.json'))
          if len(cache_paths) != expected:
              raise SystemExit(f'Expected {expected} cache files, found {len(cache_paths)}.')

          invalid = []
          pattern = re.compile(r'sbom-chunk-(\d+)\.json$')

          for path in cache_paths:
              match = pattern.search(path.name)
              if not match:
                  continue
              chunk_id = int(match.group(1))
              try:
                  with path.open(encoding='utf-8') as fh:
                      json.load(fh)
              except Exception as exc:
                  invalid.append((chunk_id, path, str(exc)))

          if not invalid:
              print('All chunk caches parsed successfully.')
          else:
              print(f'Found {len(invalid)} invalid chunk caches. Regenerating...')
              for chunk_id, cache_path, reason in invalid:
                  list_path = cache_dir / f'sbom-chunk-{chunk_id}.txt'
                  if not list_path.is_file():
                      raise SystemExit(f'Missing file list for chunk {chunk_id}: {list_path}')
                  print(f'  Regenerating chunk {chunk_id}: {reason}')
                  subprocess.run(
                      [
                          'west', 'ncs-sbom',
                          '--input-list-file', str(list_path),
                          '--license-detectors', 'scancode-toolkit',
                          '--optional-license-detectors', 'scancode-toolkit',
                          '--output-cache-database', str(cache_path),
                      ],
                      check=True,
                  )
          PY

      - id: merge
        name: Merge cache databases
        env:
          EXPECTED_CHUNKS: ${{ needs.prepare.outputs.chunk_count }}
        working-directory: ncs/nrf
        run: |
          python <<'PY'
          import glob
          import json
          import os
          import subprocess

          expected = int(os.environ['EXPECTED_CHUNKS'])
          cache_paths = sorted(glob.glob('chunk-caches/sbom-chunk-*.json'))
          if len(cache_paths) != expected:
              raise SystemExit(f'Expected {expected} cache files, found {len(cache_paths)}.')

          merged = {}
          for path in cache_paths:
              try:
                  with open(path, 'r', encoding='utf-8') as fp:
                      data = json.load(fp)
              except json.JSONDecodeError as exc:
                  with open(path, 'r', encoding='utf-8') as fp:
                      lines = fp.readlines()
                  start = max(exc.lineno - 3, 0)
                  end = min(exc.lineno + 2, len(lines))
                  snippet = ''.join(lines[start:end])
                  raise SystemExit(f'Failed to parse {path}: {exc}\\nContext:\\n{snippet}')
              for rel_path, payload in data.get('files', {}).items():
                  if rel_path in merged:
                      existing = merged[rel_path]
                      if existing['sha1'] != payload['sha1']:
                          raise SystemExit(f'Conflicting SHA1 for {rel_path}')
                      continue
                  merged[rel_path] = payload

          with open('sbom-cache-merged.json', 'w', encoding='utf-8') as fp:
              json.dump({'files': merged}, fp, indent=2)

          input_list = subprocess.check_output(['git', 'ls-files'], text=True).splitlines()
          with open('sbom-all-files.txt', 'w', encoding='utf-8') as fp:
              for path in input_list:
                  fp.write(f'{path}\n')
          PY

      - name: Generate final HTML and SPDX reports
        working-directory: ncs/nrf
        run: |
          west ncs-sbom \
            --input-list-file sbom-all-files.txt \
            --license-detectors cache-database \
            --optional-license-detectors cache-database \
            --input-cache-database sbom-cache-merged.json \
            --output-html sbom-report.html \
            --output-spdx sbom-report.spdx

      - name: Upload final artifacts
        uses: actions/upload-artifact@v4
        with:
          name: sbom-final-${{ github.run_id }}
          if-no-files-found: error
          retention-days: 30
          path: |
            ncs/nrf/sbom-report.html
            ncs/nrf/sbom-report.spdx
            ncs/nrf/sbom-cache-merged.json
